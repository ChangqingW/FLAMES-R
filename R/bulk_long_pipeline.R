#' Pipeline for Bulk Data
#'
#' @md
#' @description Semi-supervised isofrom detection and annotation for long read data.
#' This variant is meant for bulk samples. Specific parameters relating to
#' analysis can be changed either through function arguments, or through a
#' configuration JSON file.
#' @inherit sc_long_pipeline details description
#'
#' @return \code{bulk_long_pipeline} returns a SummarizedExperiment object, containing a count
#' matrix as an assay, gene annotations under metadata, as well as a list of the other
#' output files generated by the pipeline. The pipeline also outputs a number of output
#' files into the given \code{outdir} directory. These output files generated by the pipeline are:
#' \itemize{
#'  \item{transcript_count.csv.gz}{ - a transcript count matrix (also contained in the SummarizedExperiment)}
#'  \item{isoform_annotated.filtered.gff3}{ - isoforms in gff3 format (also contained in the SummarizedExperiment)}
#'  \item{transcript_assembly.fa}{ - transcript sequence from the isoforms}
#'  \item{align2genome.bam}{ - sorted BAM file with reads aligned to genome}
#'  \item{realign2transcript.bam}{ - sorted realigned BAM file using the transcript_assembly.fa as reference}
#'  \item{tss_tes.bedgraph}{ - TSS TES enrichment for all reads (for QC)}
#' }
#'
#' @param fastq the path to the directory containing the fastq input files to merge into one, `merged.fastq.gz`. If `merged.fastq.gz` already
#' exists, the fastq files are not merged and the existing merged file is used.
#' @param in_bam optional BAM file path which replaces fastq directory argument. This skips the genome alignment and
#' realignment steps
#' @inheritParams sc_long_pipeline
#'
#' @seealso
#' [sc_long_pipeline()] for single cell data,
#' [SummarizedExperiment()] for how data is outputted
#'
#' @example inst/examples/pipeline_example.R
#' @importFrom SummarizedExperiment SummarizedExperiment rowData colData rowData<- colData<- rowRanges rowRanges<-
#' @importFrom utils read.csv read.table
#' @importFrom dplyr group_by summarise_at slice_max filter
#' @importFrom magrittr "%>%"
#' @importFrom BiocGenerics cbind colnames rownames start end
#' @export
bulk_long_pipeline <-
    function(annot,
             fastq,
             in_bam = NULL,
             outdir,
             genome_fa,
             minimap2_dir = "",
             downsample_ratio = 1,
             config_file = NULL,
             do_genome_align = TRUE,
             do_isoform_id = TRUE,
             do_read_realign = TRUE,
             do_transcript_quanti = TRUE,
             gen_raw_isoform = TRUE,
             has_UMI = FALSE,
             MAX_DIST = 10,
             MAX_TS_DIST = 100,
             MAX_SPLICE_MATCH_DIST = 10,
             min_fl_exon_len = 40,
             Max_site_per_splice = 3,
             Min_sup_cnt = 10,
             Min_cnt_pct = 0.01,
             Min_sup_pct = 0.2,
             strand_specific = 1,
             remove_incomp_reads = 5,
             use_junctions = TRUE,
             no_flank = TRUE,
             use_annotation = TRUE,
             min_tr_coverage = 0.75,
             min_read_coverage = 0.75) {
        # filenames for internal steps
        merged_fq <- paste(outdir, "merged.fastq.gz", sep = "/")
        # bc_file <- paste(outdir, "pseudo_barcode_annotation.csv", sep="/")


        check_return <- check_arguments(
            annot,
            fastq,
            in_bam,
            outdir,
            genome_fa,
            minimap2_dir,
            downsample_ratio,
            config_file,
            do_genome_align,
            do_isoform_id,
            do_read_realign,
            do_transcript_quanti,
            gen_raw_isoform,
            has_UMI,
            MAX_DIST,
            MAX_TS_DIST,
            MAX_SPLICE_MATCH_DIST,
            min_fl_exon_len,
            Max_site_per_splice,
            Min_sup_cnt,
            Min_cnt_pct,
            Min_sup_pct,
            strand_specific,
            remove_incomp_reads,
            use_junctions,
            no_flank,
            use_annotation,
            min_tr_coverage,
            min_read_coverage
        )

        config_file <- check_return$config

        # create output directory if one doesn't exist
        if (!dir.exists(outdir)) {
            cat("Output directory does not exists: one is being created\n")
            dir.create(outdir)
            print(outdir)
        }

        if (file.exists(merged_fq)) {
            cat(merged_fq, " already exists, no need to merge fastq files\n")
        } else {
            # this preprocessing needs only be done if we are using a fastq_dir, instead
            # of a bam file for reads,
            cat("Preprocessing bulk fastqs...\n")
            # run the merge_bulk_fastq function as preprocessing
            merge_bulk_fastq(fastq, merged_fq)
        }

        out_files <-
            generic_long_pipeline(
                annot,
                merged_fq,
                in_bam,
                outdir,
                genome_fa,
                minimap2_dir,
                downsample_ratio,
                config_file,
                do_genome_align,
                do_isoform_id,
                do_read_realign,
                do_transcript_quanti,
                gen_raw_isoform,
                has_UMI,
                MAX_DIST,
                MAX_TS_DIST,
                MAX_SPLICE_MATCH_DIST,
                min_fl_exon_len,
                Max_site_per_splice,
                Min_sup_cnt,
                Min_cnt_pct,
                Min_sup_pct,
                strand_specific,
                remove_incomp_reads,
                use_junctions,
                no_flank,
                use_annotation,
                min_tr_coverage,
                min_read_coverage
            )

        se <- generate_bulk_summarized(out_files)

        # return the created summarizedexperiment
        se
    }

generate_bulk_summarized <- function(out_files) {
    mdata <- list(
        "OutputFiles" = out_files
    )

    transcript_count <- read.csv(out_files$counts, stringsAsFactors = FALSE)
    isoform_FSM_annotation <- read.csv(file.path(out_files$outdir, "isoform_FSM_annotation.csv"), stringsAsFactors = FALSE)

    transcript_count <- transcript_count[match(isoform_FSM_annotation$transcript_id, transcript_count$transcript_id), ]
    transcript_count$FSM_match <- isoform_FSM_annotation$FSM_match
    sample_bcs <- colnames(transcript_count)[!(colnames(transcript_count) %in% c("transcript_id", "gene_id", "FSM_match"))]
    tr_anno <- transcript_count[, c("transcript_id", "gene_id", "FSM_match")]

    # sum transcript (FSM) counts
    mer_tmp <- transcript_count %>%
        group_by(FSM_match) %>%
        summarise_at(sample_bcs, sum)

    # Create long read SCE
    tr_anno <- tr_anno[match(mer_tmp$FSM_match, tr_anno$FSM_match), ]
    tr_se <- SummarizedExperiment::SummarizedExperiment(
        assays = list(counts = as.matrix(mer_tmp[, -1])),
        metadata = mdata
    )

    # Annotation for rowRanges
    isoform_gff <- rtracklayer::import.gff3(out_files$isoform_annotated)
    isoform_gff$Parent <- as.character(isoform_gff$Parent)
    isoform_gff$transcript_id <- unlist(lapply(strsplit(isoform_gff$Parent, split = ":"), function(x) {
        x[2]
    }))
    isoform_gff <- S4Vectors::split(isoform_gff, isoform_gff$transcript_id)

    rownames(tr_se) <- mer_tmp$FSM_match
    rowData(tr_se) <- DataFrame(tr_anno)
    rowRanges(tr_se) <- isoform_gff[rowData(tr_se)$transcript_id]
    # return the created singlecellexperiment
    tr_se
}

#' @export
create_se_from_dir <- function(outdir) {
    out_files <- list(
        counts = file.path(outdir, "transcript_count.csv.gz"),
        isoform_annotated = file.path(outdir, "isoform_annotated.filtered.gff3"),
        outdir = outdir,
        transcript_assembly = file.path(outdir, "transcript_assembly.fa"),
        align_bam = file.path(outdir, "align2genome.bam"),
        realign2transcript = file.path(outdir, "realign2transcript.bam"),
        tss_tes = file.path(outdir, "tss_tes.bedgraph")
    )
    return(generate_bulk_summarized(out_files))
}