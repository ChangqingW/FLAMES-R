#' Pipeline for Bulk Data
#'
#' @md
#' @description
#' Semi-supervised isofrom detection and annotation for long read data.
#' This variant is meant for bulk samples. Specific parameters relating to
#' analysis can be changed either through function arguments, or through a
#' configuration JSON file.
#'
#' @details The default parameters can be changed either through the function
#' arguments are through the configuration JSON file \code{config_file}. the \code{pipeline_parameters}
#' section specifies which steps are to be executed in the pipeline - by default, all
#' steps are executed. The \code{isoform_parameters} section affects isoform detection - key
#' parameters include:
#' \itemize{
#'  \item{\code{Min_sup_cnt}}{ which causes transcripts with less reads aligned than
#' it's value to be discarded}
#'  \item{\code{MAX_TS_DIST}}{ which merges transcripts with the same intron
#' chain and TSS/TES distace less than \code{MAX_TS_DIST}}
#'  \item{\code{strand_specific}}{ which specifies if reads are in the same strand as the mRNA (1),
#' or the reverse complemented (-1) or not strand specific (0), which results in
#' strand information being based on reference annotation.}
#' }
#'
#' @return \code{bulk_long_pipeline} returns a SummarizedExperiment object, containing a count
#' matrix as an assay, gene annotations under metadata, as well as a list of the other
#' output files generated by the pipeline. The pipeline also outputs a number of output
#' files into the given \code{outdir} directory. These output files generated by the pipeline are:
#' \itemize{
#'  \item{transcript_count.csv.gz}{ - a transcript count matrix (also contained in the SummarizedExperiment)}
#'  \item{isoform_annotated.filtered.gff3}{ - isoforms in gff3 format (also contained in the SummarizedExperiment)}
#'  \item{transcript_assembly.fa}{ - transcript sequence from the isoforms}
#'  \item{align2genome.bam}{ - sorted BAM file with reads aligned to genome}
#'  \item{realign2transcript.bam}{ - sorted realigned BAM file using the transcript_assembly.fa as reference}
#'  \item{tss_tes.bedgraph}{ - TSS TES enrichment for all reads (for QC)}
#' }
#'
#' @param fastq the directory containing the fastq input files to merge into one, `merged.fastq.gz`. If `merged.fastq.gz` already
#' exists, the fastq files are not merged and the existing merged file is used.
#' @param in_bam optional BAM file which replaces fastq directory argument. This skips the genome alignment and
#' realignment steps
#' @inheritParams sc_long_pipeline
#' @seealso
#' [sc_long_pipeline()] for single cell data,
#' [SummarizedExperiment()] for how data is outputted
#'
#' @examples
#' # this example works fine, but because the package only includes a small subset of the fastq files,
#' # it doesn't work because it at some point produces a file that is blank. Fix? use biocFileCache
#' # To run this example, we require the full example fastq dataset, available on Zenodo
#' # download the two fastq files, move them to a folder to be merged together
#' temp_path <- tempfile()
#' bfc <- BiocFileCache::BiocFileCache(temp_path, ask=FALSE)
#' file_url <- 
#'     "https://raw.githubusercontent.com/OliverVoogd/FlamesR/master/inst/data"
#' # download the required fastq files, and move them to new folder
#' fastq1 <- bfc[[names(BiocFileCache::bfcadd(bfc, "Fastq1", paste(file_url, "fastq/sample1.fastq.gz", sep="/")))]]
#' fastq2 <- bfc[[names(BiocFileCache::bfcadd(bfc, "Fastq2", paste(file_url, "fastq/sample2.fastq.gz", sep="/")))]]
#' fastq_dir <- paste(temp_path, "fastq_dir", sep="/") # the downloaded fastq files need to be in a directory to be merged together
#' dir.create(fastq_dir)
#' file.copy(c(fastq1, fastq2), fastq_dir)
#' unlink(c(fastq1, fastq2)) # the original files can be deleted
#' 
#' # run the FLAMES bulk pipeline, using the downloaded files
#' \dontrun{
#' se <- bulk_long_pipeline(annot=system.file("extdata/SIRV_anno.gtf", package="FLAMES"), 
#'                    fastq=fastq_dir,
#'                    outdir=tempdir(), genome_fa=system.file("extdata/SIRV_genomefa.fasta", package="FLAMES"),
#'                    config_file=system.file("extdata/SIRV_config_default.json", package="FLAMES"))
#' }
#' @importFrom SummarizedExperiment SummarizedExperiment
#' @importFrom utils read.csv read.table
#' @importFrom BiocFileCache BiocFileCache
#' @export
bulk_long_pipeline <- function(annot, fastq, in_bam=NULL, outdir, genome_fa,
                                minimap2_dir=NULL, downsample_ratio=1, config_file=NULL,
                                do_genome_align=TRUE, do_isoform_id=TRUE,
                                do_read_realign=TRUE, do_transcript_quanti=TRUE,
                                gen_raw_isoform=TRUE, has_UMI=FALSE,
                                MAX_DIST=10, MAX_TS_DIST=100, MAX_SPLICE_MATCH_DIST=10,
                                min_fl_exon_len=40, Max_site_per_splice=3, Min_sup_cnt=10,
                                Min_cnt_pct=0.01, Min_sup_pct=0.2, strand_specific=1, remove_incomp_reads=5,
                                use_junctions=TRUE, no_flank=TRUE,
                                use_annotation=TRUE, min_tr_coverage=0.75, min_read_coverage=0.75) {

    # filenames for internal steps
    infq <- paste(outdir, "merged.fastq.gz", sep="/")
    bc_file <- paste(outdir, "pseudo_barcode_annotation.csv", sep="/")

    # create output directory if one doesn't exist
    if (!dir.exists(outdir)) {
        cat("Output directory does not exists: one is being created\n")
        dir.create(outdir)
        print(outdir)
    }

    if (is.null(in_bam)) {
        # use existing merge fastq if already exists
        if (file.exists(infq)) {
            cat(infq, " already exists, no need to merge fastq files\n")
        } else {
            # this preprocessing needs only be done if we are using a fastq_dir, instead
            # of a bam file for reads,
            cat("Preprocessing bulk fastqs...\n")
            # run the merge_bulk_fastq function as preprocessing
            merge_bulk_fastq(fastq, bc_file, infq)
        }
    } else {
        bc_file = NULL;
        fastq=NULL;
    }
    generic_long_pipeline(annot, infq, in_bam, outdir, genome_fa,
    #generic_long_pipeline(annot, infq, outdir, genome_fa,
                minimap2_dir, downsample_ratio, config_file,
                do_genome_align, do_isoform_id,
                do_read_realign, do_transcript_quanti,
                gen_raw_isoform, has_UMI,
                MAX_DIST, MAX_TS_DIST, MAX_SPLICE_MATCH_DIST,
                min_fl_exon_len, Max_site_per_splice, Min_sup_cnt,
                Min_cnt_pct, Min_sup_pct, strand_specific, remove_incomp_reads,
                use_junctions, no_flank,
                use_annotation, min_tr_coverage, min_read_coverage,
                bc_file);


    se <- generate_bulk_summarized(outdir)

    # return the created summarizedexperiment
    se
}

generate_bulk_summarized <- function(outdir) {
    counts <- read.csv(paste0(outdir, "/transcript_count.csv.gz"))
    annot <- read.table(paste0(outdir, "/isoform_annotated.filtered.gff3"))
    colnames(annot) <- c("SequenceID", "Source", "Feature", "Start", "End", "Score", "Strand", "Phase", "Attributes")
    mdata <- list(
            "Annotations"=annot,
            "OutputFiles"=
                        list("transcript_assembly"=paste0(outdir, "/transcript_assembly.fa"),
                            "align2genome"=paste0(outdir, "/align2genome.bam"),
                            "realign2transcript"=paste0(outdir, "/realign2transcript.bam"),
                            "tss_tes"=paste0(outdir, "/tss_tes.bedgraph")
                            )
            )
    se <- SummarizedExperiment::SummarizedExperiment(list("Flames Bulk"=counts),
                                metadata=mdata)
    
    se
}